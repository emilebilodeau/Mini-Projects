{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `nltk` we can download translated sentences between different languages. You can see the example between **English and French** below but feel free to try different combination as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, Reshape, GRU, Dropout, \\\n",
    "TimeDistributed\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to /home/emile/nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('comtrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import comtrans\n",
    "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: align Pages: 1 -->\n",
       "<svg width=\"342pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 341.50 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>align</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 337.5,-112 337.5,4 -4,4\"/>\n",
       "<!-- Resumption_source -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Resumption_source</title>\n",
       "<text text-anchor=\"middle\" x=\"51\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Resumption</text>\n",
       "</g>\n",
       "<!-- of_source -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>of_source</title>\n",
       "<text text-anchor=\"middle\" x=\"147\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">of</text>\n",
       "</g>\n",
       "<!-- Resumption_source&#45;&#45;of_source -->\n",
       "<!-- Reprise_target -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Reprise_target</title>\n",
       "<text text-anchor=\"middle\" x=\"59\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Reprise</text>\n",
       "</g>\n",
       "<!-- Resumption_source&#45;&#45;Reprise_target -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Resumption_source&#45;&#45;Reprise_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.98,-71.7C54.22,-60.85 55.81,-46.92 57.05,-36.1\"/>\n",
       "</g>\n",
       "<!-- the_source -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>the_source</title>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">the</text>\n",
       "</g>\n",
       "<!-- of_source&#45;&#45;the_source -->\n",
       "<!-- de_target -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>de_target</title>\n",
       "<text text-anchor=\"middle\" x=\"147\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">de</text>\n",
       "</g>\n",
       "<!-- of_source&#45;&#45;de_target -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>of_source&#45;&#45;de_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147,-71.7C147,-60.85 147,-46.92 147,-36.1\"/>\n",
       "</g>\n",
       "<!-- session_source -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>session_source</title>\n",
       "<text text-anchor=\"middle\" x=\"299\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">session</text>\n",
       "</g>\n",
       "<!-- the_source&#45;&#45;session_source -->\n",
       "<!-- la_target -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>la_target</title>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">la</text>\n",
       "</g>\n",
       "<!-- the_source&#45;&#45;la_target -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>the_source&#45;&#45;la_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219,-71.7C219,-60.85 219,-46.92 219,-36.1\"/>\n",
       "</g>\n",
       "<!-- session_target -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>session_target</title>\n",
       "<text text-anchor=\"middle\" x=\"299\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">session</text>\n",
       "</g>\n",
       "<!-- session_source&#45;&#45;session_target -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>session_source&#45;&#45;session_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299,-71.7C299,-60.85 299,-46.92 299,-36.1\"/>\n",
       "</g>\n",
       "<!-- Reprise_target&#45;&#45;de_target -->\n",
       "<!-- de_target&#45;&#45;la_target -->\n",
       "<!-- la_target&#45;&#45;session_target -->\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "AlignedSent(['Resumption', 'of', 'the', 'session'], ['Reprise', 'de', 'la', 'session'], Alignment([(0, 0), (1, 1), (2, 2), (3, 3)]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comtrans.aligned_sents('alignment-en-fr.txt')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = comtrans.words('alignment-en-fr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption\n",
      "of\n",
      "the\n",
      "session\n",
      "I\n",
      "declare\n",
      "resumed\n",
      "the\n",
      "session\n",
      "of\n",
      "the\n",
      "European\n",
      "Parliament\n",
      "adjourned\n",
      "on\n",
      "Friday\n",
      "17\n",
      "December\n",
      "1999\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "for word in words[:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n",
      "Reprise de la session\n"
     ]
    }
   ],
   "source": [
    "als_0 = comtrans.aligned_sents(\"alignment-en-fr.txt\")[0]\n",
    "als_0\n",
    "\n",
    "print(\" \".join(als_0.words))\n",
    "print(\" \".join(als_0.mots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 0), (1, 1), (2, 2), (3, 3)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_0.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: align Pages: 1 -->\n",
       "<svg width=\"342pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 341.50 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>align</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 337.5,-112 337.5,4 -4,4\"/>\n",
       "<!-- Reprise_source -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Reprise_source</title>\n",
       "<text text-anchor=\"middle\" x=\"59\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Reprise</text>\n",
       "</g>\n",
       "<!-- de_source -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>de_source</title>\n",
       "<text text-anchor=\"middle\" x=\"147\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">de</text>\n",
       "</g>\n",
       "<!-- Reprise_source&#45;&#45;de_source -->\n",
       "<!-- Resumption_target -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Resumption_target</title>\n",
       "<text text-anchor=\"middle\" x=\"51\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Resumption</text>\n",
       "</g>\n",
       "<!-- Reprise_source&#45;&#45;Resumption_target -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Reprise_source&#45;&#45;Resumption_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.02,-71.7C55.78,-60.85 54.19,-46.92 52.95,-36.1\"/>\n",
       "</g>\n",
       "<!-- la_source -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>la_source</title>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">la</text>\n",
       "</g>\n",
       "<!-- de_source&#45;&#45;la_source -->\n",
       "<!-- of_target -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>of_target</title>\n",
       "<text text-anchor=\"middle\" x=\"147\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">of</text>\n",
       "</g>\n",
       "<!-- de_source&#45;&#45;of_target -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>de_source&#45;&#45;of_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147,-71.7C147,-60.85 147,-46.92 147,-36.1\"/>\n",
       "</g>\n",
       "<!-- session_source -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>session_source</title>\n",
       "<text text-anchor=\"middle\" x=\"299\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">session</text>\n",
       "</g>\n",
       "<!-- la_source&#45;&#45;session_source -->\n",
       "<!-- the_target -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>the_target</title>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">the</text>\n",
       "</g>\n",
       "<!-- la_source&#45;&#45;the_target -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>la_source&#45;&#45;the_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219,-71.7C219,-60.85 219,-46.92 219,-36.1\"/>\n",
       "</g>\n",
       "<!-- session_target -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>session_target</title>\n",
       "<text text-anchor=\"middle\" x=\"299\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">session</text>\n",
       "</g>\n",
       "<!-- session_source&#45;&#45;session_target -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>session_source&#45;&#45;session_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299,-71.7C299,-60.85 299,-46.92 299,-36.1\"/>\n",
       "</g>\n",
       "<!-- Resumption_target&#45;&#45;of_target -->\n",
       "<!-- of_target&#45;&#45;the_target -->\n",
       "<!-- the_target&#45;&#45;session_target -->\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "AlignedSent(['Reprise', 'de', 'la', 'session'], ['Resumption', 'of', 'the', 'session'], Alignment([(0, 0), (1, 1), (2, 2), (3, 3)]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_0.invert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import Alignment, AlignedSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating own alignments, no need though?\n",
    "als = AlignedSent( [\"Reprise\", \"de\", \"la\", \"session\" ], \\\n",
    "    [\"Resumption\", \"of\", \"the\", \"session\" ] , \\\n",
    "    Alignment( [ (0 , 0), (1 , 1), (2 , 2), (3 , 3) ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reprise', 'de', 'la', 'session']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_0.mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = comtrans.aligned_sents('alignment-en-fr.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_words = []\n",
    "for i in range(len(als)):\n",
    "    for word in als[i].mots:\n",
    "        french_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734360\n"
     ]
    }
   ],
   "source": [
    "print(len(french_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_words = [word.lower() for word in french_words]\n",
    "set_french = set(french_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22819"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sentences = []\n",
    "for i in range(len(als)):\n",
    "    sent = ' '.join(als[i].mots)\n",
    "    french_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reprise de la session'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = []\n",
    "for i in range(len(als)):\n",
    "    for word in als[i].words:\n",
    "        english_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = [word.lower() for word in english_words]\n",
    "set_english = set(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17075"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = []\n",
    "for i in range(len(als)):\n",
    "    sent = ' '.join(als[i].words)\n",
    "    english_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption of the session',\n",
       " 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .',\n",
       " 'You have requested a debate on this subject in the course of the next few days , during this part-session .']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentences = english_sentences[:3]\n",
    "example_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'of': 2, 'session': 3, 'you': 4, 'a': 5, 'i': 6, 'on': 7, 'in': 8, 'this': 9, 'resumption': 10, 'declare': 11, 'resumed': 12, 'european': 13, 'parliament': 14, 'adjourned': 15, 'friday': 16, '17': 17, 'december': 18, '1999': 19, 'and': 20, 'would': 21, 'like': 22, 'once': 23, 'again': 24, 'to': 25, 'wish': 26, 'happy': 27, 'new': 28, 'year': 29, 'hope': 30, 'that': 31, 'enjoyed': 32, 'pleasant': 33, 'festive': 34, 'period': 35, 'have': 36, 'requested': 37, 'debate': 38, 'subject': 39, 'course': 40, 'next': 41, 'few': 42, 'days': 43, 'during': 44, 'part': 45}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input: Resumption of the session\n",
      "  Ouput: [10, 2, 1, 3]\n",
      "Sequence 2 in x\n",
      "  Input: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .\n",
      "  Ouput: [6, 11, 12, 1, 3, 2, 1, 13, 14, 15, 7, 16, 17, 18, 19, 20, 6, 21, 22, 23, 24, 25, 26, 4, 5, 27, 28, 29, 8, 1, 30, 31, 4, 32, 5, 33, 34, 35]\n",
      "Sequence 3 in x\n",
      "  Input: You have requested a debate on this subject in the course of the next few days , during this part-session .\n",
      "  Ouput: [4, 36, 37, 5, 38, 7, 9, 39, 8, 1, 40, 2, 1, 41, 42, 43, 44, 9, 45, 3]\n"
     ]
    }
   ],
   "source": [
    "text_tokenized, text_tokenizer = tokenize(example_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(example_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input: {}'.format(sent))\n",
    "    print('  Ouput: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to tokenize the everything. Will have to use this function to tokenize english_sentences and french_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text_tokenized, english_text_tokenizer = tokenize(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_text_tokenized, french_text_tokenizer = tokenize(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [10  2  1  3]\n",
      "  Output: [10  2  1  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Sequence 2 in x\n",
      "  Input:  [ 6 11 12  1  3  2  1 13 14 15  7 16 17 18 19 20  6 21 22 23 24 25 26  4\n",
      "  5 27 28 29  8  1 30 31  4 32  5 33 34 35]\n",
      "  Output: [ 6 11 12  1  3  2  1 13 14 15  7 16 17 18 19 20  6 21 22 23 24 25 26  4\n",
      "  5 27 28 29  8  1 30 31  4 32  5 33 34 35]\n",
      "Sequence 3 in x\n",
      "  Input:  [ 4 36 37  5 38  7  9 39  8  1 40  2  1 41 42 43 44  9 45  3]\n",
      "  Output: [ 4 36 37  5 38  7  9 39  8  1 40  2  1 41 42 43 44  9 45  3  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to tokenize both english and french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_english = pad(english_text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_french = pad(french_text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing max length\n",
    "def pad(x, length=20):\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_english_sentences, preproc_french_sentences, \\\n",
    "english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English sentence length: 20\n",
      "Max French sentence length: 20\n",
      "English vocabulary size:  15930\n",
      "French vocabulary size:  21870\n"
     ]
    }
   ],
   "source": [
    "print('Max English sentence length:', max_english_sequence_length)\n",
    "print('Max French sentence length:', max_english_sequence_length)\n",
    "print('English vocabulary size: ', english_vocab_size)\n",
    "print('French vocabulary size: ', french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenier.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "from tensorflow.keras.backend import set_session\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "config_proto = tf.ConfigProto()\n",
    "off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "config_proto.graph_options.rewrite_options.arithmetic_optimization = off\n",
    "session = tf.Session(config=config_proto)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape, output_sequence_length, english_vocab_size,\n",
    "         french_vocab_size):\n",
    "    learning_rate = 0.005\n",
    "    # layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # compile\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                 optimizer=Adam(learning_rate),\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33334, 20)\n",
      "(33334, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "print(tmp_x.shape)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "print(tmp_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33334, 20, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_french_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "rnn_model = model(tmp_x.shape, max_french_sequence_length, english_vocab_size,\n",
    "                 french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 20, 256)           198144    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 20, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 21870)         22416750  \n",
      "=================================================================\n",
      "Total params: 22,878,062\n",
      "Trainable params: 22,878,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26667 samples, validate on 6667 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Resource __per_step_22/training/gradients/AddN_5/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node training/gradients/AddN_5/tmp_var}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-b14c1608b257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=2, epochs=10,\n\u001b[0;32m----> 2\u001b[0;31m               validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bootcamp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/bootcamp_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bootcamp_env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/bootcamp_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_22/training/gradients/AddN_5/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node training/gradients/AddN_5/tmp_var}}]]"
     ]
    }
   ],
   "source": [
    "rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=2, epochs=10,\n",
    "              validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
